install.packages("randomForest")
library(randomForest)
set.seed(123)
train <- read.csv("~/Downloads/train.csv")
View(train)
View(train)
fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp
;
FamilyID2, data=train, importance=T, ntree=2000)
data=train, importance=T, ntree=2000)
data=train, importance=T, ntree=2000)
fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, importance=T, ntree=2000)
train <- complete.cases(train)
train <- read.csv("~/Downloads/train.csv")
train <- train[complete.cases(train), ]
View(train)
fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, importance=T, ntree=2000)
varImpPlot(fit)
system.time(fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train, importance=T, ntree=2000))
set.seed(123)
mortDefault <- read.csv("/Users/markmo/Downloads/mortDefault/mortDefault2009.csv")
in_train <- createDataParition(mortDefault$default, p=0.6, list=F)
train <- mortDefault[in_train, ]
test <- mortdefault[-in_train, ]
system.time(model <- randomForest(default ~ creditScore + houseAge + yearsEmployed + ccDebt, data=train, importance=T, ntree=10))
install.packages("caret")
library(caret)
set.seed(123)
in_train <- createDataParition(mortDefault$default, p=0.6, list=F)
in_train <- createDataPartition(mortDefault$default, p=0.6, list=F)
train <- mortDefault[in_train, ]
test <- mortdefault[-in_train, ]
system.time(model <- randomForest(default ~ creditScore + houseAge + yearsEmployed + ccDebt, data=train, importance=T, ntree=10))
test <- mortDefault[-in_train, ]
system.time(model <- randomForest(default ~ creditScore + houseAge + yearsEmploy + ccDebt, data=train, importance=T, ntree=10))
varImpPlot(model)
q()
library(randomForest)
set.seed(123)
mortDefault <- read.csv("/Users/markmo/Downloads/mortDefault/mortDefault2009.csv")
View(mortDefault)
q()
install.packages("rJava")
version
install.packages("RCurl")
library(RCurl)
install.packages("devtools")
library(devtools)
library(SparkR)
install_github("amplab-extras/SparkR-pkg", subdir="pkg")
library(SparkR)
Sys.setenv(SPARK_MEM="1g")
sc <- sparkR.init(master="local[*]")
dictionary <- textFile(sc, "hdfs://lab/cxp/dictionary.csv")
dictionary <- textFile(sc, "hdfs://localhost:9000/lab/cxp/dictionary.csv")
q()
Sys.setenv("SPARK_VERSION=1.3.0")
Sys.setenv(SPARK_VERSION="1.3.0")
Sys.setenv(SPARK_HADOOP_VERSION="2.6.0")
library(devtools)
install_github("repo/SparkR-pkg", ref="branchname", subdir="pkg")
install_github("amplab-extras/SparkR-pkg", ref="master", subdir="pkg")
library(SparkR)
sc <- sparkR.init(master="local[*]")
dictionary <- textFile(sc, "hdfs://localhost:9000/lab/cxp/dictionary.csv")
dictionary
take(data, 2)
take(dictionary, 2)
take(dictionary, 3)
data <- textFile(sc, "hdfs://localhost:9000/lab/cxp/output/features")
take(data, 10)
count(data)
typeof(take(data, 2)[[1]])
parseFields <- function(record) {}
parseFields <- function(record) {
Sys.setlocale("LC_ALL", "C")
parts <- strsplit(record, ",")[[1]]
list(customerId=parts[1], webRequest=parts[2], pageViewsPerSession=parts[3], sessionLengthSeconds=parts[4])
}
parsedRDD <- lapply(data, parseFields)
cache(parsedRDD)
count(parsedRDD)
ipAddresses <- lapply(parsedRDD, function(x) { x$customerId})
take(ipAddresses, 10)
take(data, 10)
takes(parsedRDD, 10)
take(parsedRDD, 10)
data <- textFile(sc, "hdfs://localhost:9000/lab/cxp/output/features")
parseFields <- function(record) {
Sys.setlocale("LC_ALL", "C") # necessary for strsplit() to work correctly
parts <- strsplit(record, ",")[[1]]
list(customerId=parts[1], webRequest=parts[2], pageViewsPerSession=parts[3], sessionLengthSeconds=parts[4], averageSessionLengthSeconds=parts[5])
}
parsedRDD <- lapply(data, parseFields)
cache(parsedRDD)
take(parsedRDD, 5)
take(parsedRDD[-order(parsedRDD$averageSessionLengthSeconds),])
take(parsedRDD[-order(parsedRDD[[averageSessionLengthSeconds]]),])
take(parsedRDD[-order(parsedRDD[[5]]),])
parsedRDD$averageSessionLengthSeconds
take(sortBy(parsedRDD, function(x) {x[["averageSessionLengthSeconds"]]}, 10)
;
take(sortBy(parsedRDD, function(x) {x[["averageSessionLengthSeconds"]]}), 10)
take(sortBy(parsedRDD, function(x) {-x[["averageSessionLengthSeconds"]]}), 10)
take(sortBy(parsedRDD, function(x) {-x[["averageSessionLengthSeconds"]]}, false), 10)
take(sortBy(parsedRDD, function(x) {x[["averageSessionLengthSeconds"]]}, false), 10)
take(sortBy(parsedRDD, function(x) {x[["averageSessionLengthSeconds"]]}, false), 10)
take(sortBy(parsedRDD, function(x) {x[["averageSessionLengthSeconds"]]}, F), 10)
q()
SparkR.close()
q()
setwd("/Users/markmo/src/Tutorials/SparkR/demo/src/")
library(SparkR)
sc <- sparkR.init(master="local[*]")
sqlCtx <- sparkRSQL.init(sc)
df <- jsonFile(sqlCtx, "../data/people.json")
head(df)
avg <- select(df, avg(df$age))
head(avg)
txnsRaw <- loadDF(sqlCtx, paste(getwd(), "/../data/Customer_Transactions.parquet", sep = ""), "parquet")
demo <- withColumnRenamed(loadDF(sqlCtx, paste(getwd(), "/../data/Customer_Demographics.parquet", sep = ""), "parquet"),
"cust_id", "ID")
sample <- loadDF(sqlCtx, paste(getwd(), "/../data/DM_Sample.parquet", sep = ""), "parquet")
printSchema(txnsRaw)
printSchema(demo)
printSchema(sample)
head(txnsRaw)
perCustomer <- agg(groupBy(txnsRaw, "cust_id"),
txns = countDistinct(txnsRaw$day_num),
spend = sum(txnsRaw$extended_price))
head(perCustomer)
joinToDemo <- select(join(perCustomer, demo),
demo$"*",
perCustomer$txns,
perCustomer$spend)
joinToDemo <- select(join(perCustomer, demo, perCustomer$cust_id == demo$ID),
demo$"*",
perCustomer$txns,
perCustomer$spend)
head(joinToDemo)
trainDF <- select(join(joinToDemo, sample, joinToDemo$ID == sample$cust_id),
joinToDemo$"*",
alias(cast(sample$respondYes, "double"), "respondYes"))
head(trainDF)
estDF <- select(filter(join(joinToDemo, sample, joinToDemo$ID == sample$cust_id, "left_outer"),
"cust_id IS NULL"),
joinToDemo$"*")
head(estDF)
colnames(estDF)
printSchema(estDF)
persist(estDF, "MEMORY_ONLY")
count(estDF)
train <- collect(trainDF) ; train$ID <- NULL
est <- collect(estDF)
theModel <- glm(respondYes ~ ., "binomial", train)
predictWithID <- function(modObj, data, idField) {
scoringData <- data[, !names(data) %in% as.character(idField)]
scores <- predict(modObj, scoringData, type = "response", se.fit = TRUE)
idScores <- data.frame(ID = data[as.character(idField)], Score = scores$fit)
idScores[order( -idScores$Score), ]
}
head(predictWithID)
testScores <- predictWithID(theModel, est, "ID")
head(testScores, 25)
head(data[, !names(data) %in% as.character(idField)])
head(data[, !names(data) %in% as.character("ID")])
head(est[, !names(est) %in% as.character("ID")])
scoringData <- head(est[, !names(est) %in% as.character("ID")])
scores <- predict(theModel, scoringData, type = "response",
5         se.fit = TRUE)
scores <- predict(theModel, scoringData, type = "response",se.fit = TRUE)
head(scores)
idScores <- data.frame(ID = data[as.character(idField)], Score = scores$fit)
idScores <- data.frame(ID = data[as.character("ID")], Score = scores$fit)
idScores <- data.frame(ID = data[as.character("ID")], Score = scores$fit)
sc
library(SparkR)
sc
lines <- textFile(sc, "hdfs://localhost:9000/shakespeare.txt")
take(lines, 2)
words <- flatMap(lines, function(line) { strsplit(as.character(line), " ")[[1]] })
wordCount <- lapply(words, function(word) { list(word, 1) })
counts <- reduceByKey(wordCount, "+", 2L)
output <- collect(counts)
head(output)
df <- data.frame(matrix(unlist(output), ncol=2, byrow=T))
colnames(df) <- c("word", "freq")
df$word <- as.character(df$word)
df$freq <- as.integer(levels(df$freq))[df$freq]
sorted <- df[order(-df$freq),]
head(sorted)
sqlCtx <- sparkRSQL.init(sc)
df <- jsonFile(sqlCtx, "../data/people.json")
head(df)
avg <- select(df, avg(df$age))
head(avg)
txnsRaw <- loadDF(sqlCtx, paste(getwd(), "/../data/Customer_Transactions.parquet", sep = ""), "parquet")
head(txnsRaw)
printSchema(txnsRaw)
demo <- withColumnRenamed(loadDF(sqlCtx, paste(getwd(), "/../data/Customer_Demographics.parquet", sep = ""), "parquet"),
"cust_id", "ID")
sample <- loadDF(sqlCtx, paste(getwd(), "/../data/DM_Sample.parquet", sep = ""), "parquet")
printSchema(demo)
printSchema(sample)
perCustomer <- agg(groupBy(txnsRaw, "cust_id"),
txns = countDistinct(txnsRaw$day_num),
spend = sum(txnsRaw$extended_price))
head(perCustomer)
joinToDemo <- select(join(perCustomer, demo),
demo$"*",
perCustomer$txns,
perCustomer$spend)
explain(joinToDemo)
joinToDemo <- select(join(perCustomer, demo, perCustomer$cust_id == demo$ID),
demo$"*",
perCustomer$txns,
perCustomer$spend)
explain(joinToDemo)
trainDF <- select(join(joinToDemo, sample, joinToDemo$ID == sample$cust_id),
joinToDemo$"*",
alias(cast(sample$respondYes, "double"), "respondYes"))
estDF <- select(filter(join(joinToDemo, sample, joinToDemo$ID == sample$cust_id, "left_outer"),
"cust_id IS NULL"),
joinToDemo$"*")
persist(estDF, "MEMORY_ONLY")
train <- collect(trainDF) ; train$ID <- NULL
est <- collect(estDF)
theModel <- glm(respondYes ~ ., "binomial", train)
summary(theModel)
predictWithID <- function(modObj, data, idField) {
scoringData <- data[, !names(data) %in% as.character(idField)]
scores <- predict(modObj, scoringData, type = "response", se.fit = TRUE)
idScores <- data.frame(ID = data[as.character(idField)], Score = scores$fit)
idScores[order( -idScores$Score), ]
}
testScores <- predictWithID(theModel, est, "ID")
head(testScores, 25)
q()
